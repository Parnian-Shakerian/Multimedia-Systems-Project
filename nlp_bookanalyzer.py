# -*- coding: utf-8 -*-
"""BookAnalyzer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fyU4VV5kj21fdeo44vfOAd6PLTW3E1W-
"""

!pip install python-Levenshtein
!pip install gensim===3.6.0
!pip install beautifulsoup4 lxml
!pip install ebooklib

pip install transformers

pip install sumy

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

pip install requests beautifulsoup4 lxml ebooklib sumy

import requests
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
import ebooklib
from ebooklib import epub

def f(seq):
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]
# the below summary percentages can be adjusted at will
def summary(x):
    num_sentences = len(split_sentences(x))

    if num_sentences > 100:
        perc = 0.05
    elif num_sentences > 50:
        perc = 0.1
    elif num_sentences > 10:
        perc = 0.2
    else:
        return x

    test_summary = summarize(x, ratio=perc, split=True)
    test_summary = '\n'.join(map(str, f(test_summary)))

    return test_summary

def get_text_from_url(url):
    response = requests.get(url)
    response.encoding = 'utf-8'

    soup = BeautifulSoup(response.text, 'lxml')

    # Removing script and style elements
    for script in soup(["script", "style"]):
        script.decompose()

    # Extracting the main text content
    text = ' '.join(soup.stripped_strings)
    return text

def get_text_from_epub(file_path):
    book = epub.read_epub(file_path)
    text = []

    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):
        content = item.get_content().decode('utf-8')
        soup = BeautifulSoup(content, 'html.parser')
        text.append(' '.join(soup.stripped_strings))

    return "\n".join(text)

def get_text(file_or_url):
    if urlparse(file_or_url).scheme in ('http', 'https'):
        return get_text_from_url(file_or_url)
    elif file_or_url.lower().endswith('.epub'):
        return get_text_from_epub(file_or_url)
    else:
        with open(file_or_url, 'r') as f:
            return f.read()

def word_count(text):
    return len(text.split())

def split_text_into_chunks(text, max_chunk_size): # Large texts content split into chunks
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) >= max_chunk_size:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

from google.colab import files

uploaded = files.upload()

file_or_url = list(uploaded.keys())[0]
print(f"Uploaded file: {file_or_url}")

mytxt = get_text(file_or_url)

num_characters = len(mytxt)
print(f"Number of characters in the original text: {num_characters}")

if file_or_url.lower().endswith('.epub') or not urlparse(file_or_url).scheme in ('http', 'https'):
    mytxt_chunks = split_text_into_chunks(mytxt, max_chunk_size=int(0.4 * num_characters)) #adjust max chunk size based on number of characters
else:
    mytxt_chunks = [mytxt]

print(f"Number of chunks created: {len(mytxt_chunks)}")

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

def summary(text, num_sentences=None):
    # Estimate number of sentences if not given
    sentences = text.split('. ')
    if not num_sentences:
        if len(sentences) > 100:
            num_sentences = max(int(len(sentences) * 0.05), 1)
        elif len(sentences) > 50:
            num_sentences = max(int(len(sentences) * 0.1), 1)
        elif len(sentences) > 10:
            num_sentences = max(int(len(sentences) * 0.2), 1)
        else:
            return text

    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = LsaSummarizer()
    summary_sentences = summarizer(parser.document, num_sentences)

    seen = set()
    summary_sentences = [str(s) for s in summary_sentences if s not in seen and not seen.add(s)]

    return '\n'.join(summary_sentences)

import nltk
nltk.download('punkt')

import nltk

nltk.download('punkt')
nltk.download('punkt_tab')

mysummary = "\n\n".join(summary(chunk) for chunk in mytxt_chunks)

print("Summary:")
print(mysummary)

original_word_count = word_count(mytxt)
summary_word_count = word_count(mysummary)

print('\nWord Count:')
print(f"Original: {original_word_count}")
print(f"Summary: {summary_word_count}")

with open("summary_output.txt", "w") as output_file:
    output_file.write("Summary:\n")
    output_file.write(mysummary)
    output_file.write("\n\nWord Count:\n")
    output_file.write(f"Original: {original_word_count}\n")
    output_file.write(f"Summary: {summary_word_count}\n")

from google.colab import files

files.download("summary_output.txt")

!pip install rake-nltk

import nltk
nltk.download('stopwords')

from rake_nltk import Rake

r = Rake()

r.extract_keywords_from_text(mysummary)
keywords = r.get_ranked_phrases()

print("Top 20 Keywords/Phrases:")
for phrase in keywords[:20]:
    print("-", phrase)

with open("keywords.txt", "w") as f:
    for phrase in keywords:
        f.write(phrase + "\n")

from google.colab import files

files.download("keywords.txt")

from collections import Counter
import re
from textblob import TextBlob
!pip install textblob

words = re.findall(r'\b\w+\b', mysummary.lower())
word_freq = Counter(words)

# Most common words (excluding common stopwords)
stopwords = set(["the","and","to","of","in","a","is","it","for","on","with","as","that","this","he","she","his","her","they","was","at"])
common_words = [(word, count) for word, count in word_freq.most_common(50) if word not in stopwords]

print("Top 20 Words (excluding stopwords):")
for word, count in common_words[:20]:
    print(f"{word}: {count}")

blob = TextBlob(mysummary)
print("\nSentiment Analysis:")
print(f"Polarity: {blob.sentiment.polarity}")  # -1 negative, 1 positive
print(f"Subjectivity: {blob.sentiment.subjectivity}")  # 0 objective, 1 subjective

num_sentences = len(re.split(r'[.!?]', mysummary))
num_words = len(words)
print("\nReading Stats:")
print(f"Number of sentences: {num_sentences}")
print(f"Number of words: {num_words}")
print(f"Average words per sentence: {num_words / num_sentences:.2f}")

import re

sentences = re.split(r'(?<=[.!?])\s+', mysummary)

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('all-MiniLM-L6-v2')

sentence_embeddings = model.encode(sentences, convert_to_tensor=True)

def semantic_answer(question, sentences, sentence_embeddings, top_n=3):
    question_embedding = model.encode(question, convert_to_tensor=True)
    scores = util.cos_sim(question_embedding, sentence_embeddings)[0]
    top_indices = scores.topk(k=top_n).indices
    return "\n".join([sentences[i] for i in top_indices])

question = "How does the old man catch fish?"
print(semantic_answer(question, sentences, sentence_embeddings))

!pip install wordcloud matplotlib

from wordcloud import WordCloud
import matplotlib.pyplot as plt

wc = WordCloud(width=800, height=400, background_color='white').generate(mysummary)

plt.figure(figsize=(15,7))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

